<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Process Management in OS</title>
    <link rel="stylesheet" href="../../../../public/style.css">
    <link rel="icon" href="../../../../public/logo/favicon_io/favicon.ico">
</head>

<body class="bg-c">
    <div id="mySidepanel" class="sidepanel">
        <a href="javascript:void(0)" class="closebtn" onclick="closeNav()">×</a>
        <a href="../index.html" class="home">back</a>
        <a href="#t1" class="link">About process management</a>
        <a href="#t2" class="link">What is process?</a>
        <a href="#t3" class="link">Process state</a>
        <a href="#t4" class="link">Process scheduling</a>
        <a href="#t16" class="link">Process queues</a>
        <a href="#t5" class="link">CPU scheduling</a>
        <a href="#t6" class="link">Purpose of scheduling algorithm</a>
        <a href="#t7" class="link">FCFS</a>
        <a href="#t8" class="link">SJF</a>
        <a href="#t9" class="link">Round Robin algorithm</a>
        <a href="#t10" class="link">Multiple-processor scheduling</a>
        <a href="#t11" class="link">Processor Affinity</a>
        <a href="#t12" class="link">Load balancing</a>
        <a href="#t13" class="link">Multicore processors</a>
        <a href="#t14" class="link">Process and Thread</a>
        <a href="#t15" class="link">Multithreading in OS</a>
        <a href="#t16" class="link">Previous year questions</a>
        <div class="botbut">
            <a href="../unit3/index.html" class="link">Next Topic &rarr;</a>
            <a href="../intro/index.html" class="link">&larr; Previous Topic</a>
        </div>
    </div>
    <div id="navbar" class="grad">
        <div>
            <div class="openbtn" onclick="openNav()">
                <div id="nav-icon1" for="nav-menu1">
                    <span></span>
                    <span></span>
                    <span></span>
                </div>
            </div>
        </div>
        <div>
            <h2>Process Management in OS</h2>
        </div>
    </div>
    <div id="t1" class="content-box">
        <h1>Process Management in OS</h1>
        <ul>
            <li>A program does nothing unless its instructions are executed by CPU.</li>
            <li>A program in execution is called a process.</li>
            <li>In order to accomplish its task, process needs the computer resources.</li>
            <li>There may exist more than one process in the system which may require the same resource at the same
                time. Therefore, the operating system has to manage all the processes and the resources in a convenient
                and efficient way.</li>
            <li>Some resources may need to be executed by one process at one time to maintain the consistency otherwise
                the system can become inconsistent, and deadlock may occur.
                <ul>
                    <li>A deadlock occurs when two or more processes are unable to proceed because they are waiting for
                        each other to release resources.</li>
                </ul>
            </li>
            <li>The operating system is responsible for the following activities in connection with Process Management:
                <ol>
                    <li>Scheduling process and threads on the CPU.</li>
                    <li>Creating and deleting both user and system processes.</li>
                    <li>Suspending and resuming processes.</li>
                    <li>Providing mechanisms for process synchronization.</li>
                    <li>Providing mechanisms for process communication.</li>
                </ol>
            </li>
        </ul>
        <div id="t2" class="wh">
            <h2>What is process?</h2>
            <ul>
                <li>A process is a <u>program in execution</u> including the current values of the program counter,
                    registers and variables.</li>
                <li>The difference between a process and a program is that the program is the group of instruction where
                    as the process is the activity. Or we can say process is an active state of program.</li>
                <li>We write our computer programs in a text file and when we execute this program, it becomes a process
                    which performs all the tasks mentioned in the program.</li>
                <li>When a program is loaded into the memory and it becomes a process, it can be divided into four
                    sections: stack, heap, text and data.</li>
                <li>A process generally also includes the process stack, which contains temporary data (such as function
                    parameters, return addresses, and local variables), and a data section, which contains global
                    variables.</li>
            </ul>
            <img src="../../images/process1.svg" alt="" class="wb">
            <ul>
                <li>Stack: The process stack contains the temporary data such as method/function, parameters, return
                    address and local variables.</li>
                <li>Heap: This is dynamically allocated memory to a process during its runtime.</li>
                <li>Text: Program counters (It stores the list of address of next instructions that has to be executed
                    after the current process) & content of processor registers.</li>
                <li>Data: This section contains the all global and static variables.</li>
            </ul>
        </div>
        <div id="t3" class="wh">
            <h2>Process States</h2>
            <ul>
                <li>When a process executes, it passes through different states.</li>
                <li>These stages may differ in different operating systems.</li>
                <li>In general, a process can have one of the following five states at a time.</li>
            </ul>
            <img src="../../images/process2.svg" alt="" class="wb">
            <ul>
                <li><b>New:</b> This is the initial state when a process is first started/created.</li>
                <li><b>Ready:</b> The process are waiting to have the processor allocated to them by the operating
                    system so that they can run.
                    <ul>
                        <li>Process may come into this state after leaving the start state or while running but be
                            interrupted by the scheduler to assign CPU to some other process.</li>
                    </ul>
                </li>
                <li><b>Running</b>: After <i>Ready</i> state, the process state is set to running and the processor
                    execute its instruction.</li>
                <li><b>Waiting:</b> Process moves into the waiting state if it needs to wait for a resource, such as
                    waiting for user input, or waiting for a file to become available.</li>
                <li><b>Terminated:</b> Once the process finishes its execution or its is terminated by the operating
                    system, it is moved to the terminated state where it waits to be removed from main memory.</li>
            </ul>
            <div class="in">
                <h3>Attributes of a process</h3>
                <ul>
                    <li>The attributes of the process are used by the operating system to create the process control
                        block (PCB) for each of them.</li>
                    <li>This is also called context of the process. Attributes which are stored in the PCB are described
                        below.
                        <ol>
                            <li><b>Process ID</b>: When a process is created, a unique id is assigned to the process
                                which is used for unique identification of the process in the system.</li>
                            <li><b>Program counter</b>: A program counter stores the address of the last instruction of
                                the process on which the process was suspended. The CPU uses this address when the
                                execution of this process is resumed.</li>
                            <li><b>Process State</b>: The process, from its creation to the completion, goes through
                                various states which are new, ready, running and waiting.</li>
                            <li><b>Priority</b>: Every process has its own priority. The process with the highest
                                priority among the process gets the CPU first. This is also stored on the process
                                control block.</li>
                            <li><b>General Purpose Registers</b>: Every process has its own set of registers which are
                                used to hold the data which is generated during the execution of the process.</li>
                            <li><b>List of open files</b>: During the execution, every process uses some files which
                                need to be present in the main memory. OS also maintains a list of open files in the
                                PCB.</li>
                            <li><b>List of open devices</b>: OS also maintain the list of all open devices which are
                                used during the execution of the process.</li>
                        </ol>
                    </li>
                </ul>
                <img src="../../images/manage1.svg" alt="" class="wb">
            </div>
        </div>
        <div id="t4" class="wh">
            <h2>Process Scheduling</h2>
            <ul>
                <li>Operating system uses various schedulers for the process scheduling described below.</li>
            </ul>
            <div class="in">
                <h3>Long term scheduler</h3>
                <ul>
                    <li>Long term scheduler is also known as job scheduler. It chooses the process from the pool
                        (secondary memory) and keeps them in the ready queue maintained in the primary memory.</li>
                    <li>This scheduler decides which programs should be executed from the pool of programs waiting in
                        the job queue.</li>
                    <li>The long-term scheduler considers factors such as system utilization, available memory, and the
                        types of programs that are currently running. It decides which programs should be admitted to
                        the system, and it loads them into memory for execution.</li>
                </ul>
            </div>
            <div class="in">
                <h3>Short term schedular</h3>
                <ul>
                    <li>Once a process is admitted to the system, the short-term scheduler (also known as the CPU
                        scheduler) takes over. </li>
                    <li>The short-term scheduler is responsible for selecting which process should be executed next from
                        the pool of processes that are currently waiting in the ready queue.</li>
                    <li>This scheduler is a faster and more frequent process than the long-term scheduler, as it
                        operates on a millisecond or nanosecond time scale. The short-term scheduler considers factors
                        such as process priority, remaining burst time, and the amount of CPU time that each process has
                        consumed so far.</li>
                </ul>
            </div>
            <div class="in">
                <h3>Medium term scheduler</h3>
                <ul>
                    <li>The medium-term scheduler is a less common type of scheduler that is used in some operating
                        systems. </li>
                    <li>The medium-term scheduler is responsible for temporarily removing processes from the memory when
                        there is a shortage of memory resources. It does this by swapping out some of the processes from
                        the memory and storing them on the hard disk. This frees up memory resources for other processes
                        to use. Later, when the swapped-out processes are needed again, the medium-term scheduler swaps
                        them back into memory.</li>
                </ul>
            </div>
        </div>
        <div id="t16" class="wh">
            <h2>Process Queues</h2>
            <ul>
                <li>The operating system manages various types of queues for each of the process states. </li>
                <li>The PCB related to the process is also stored in the queue of the same state.</li>
                <li>If the process is moved from one state to another state then its PCB is also unlinked from the
                    corresponding queue and added to the other state queue in which the transition is made.</li>
            </ul>
            <img src="../../images/manage2.svg" alt="" class="wb">
            <ol>
                <li><b>Job Queue</b>
                    <ul>
                        <li>In starting, all the process get stored in the job queue.</li>
                        <li>It is maintained in the secondary memory.</li>
                        <li>The long term scheduler (Job scheduler) picks some of the process and put them in the
                            primary memory.</li>
                    </ul>
                </li>
                <li><b>Ready Queue</b>
                    <ul>
                        <li>Ready queue is maintained in primary memory.</li>
                        <li>The short term scheduler picks the job from the ready queue and dispatch of the CPU for
                            the execution.</li>
                    </ul>
                </li>
                <li><b>Waiting Queue</b>
                    <ul>
                        <li>When the process needs some IO operation in order to complete its execution, OS Changes
                            the state of the process from running to waiting.</li>
                        <li>The context (PCB) associated with the process gets stored on the waiting queue which
                            will be used by the processor when the process finishes the IO.</li>
                    </ul>
                </li>
            </ol>
        </div>
        <div class="wh">
            <h2>Various Times related to Process:</h2>
            <img src="../../images/manage3.svg" alt="" class="wb">
            <ol>
                <li><b>Arrival Time</b>
                    <ul>
                        <li>The time at which the process enters into the ready queue.</li>
                    </ul>
                </li>
                <li><b>Burst Time</b>
                    <ul>
                        <li>The total amount of time required by the CPU to execute the whole process is called the
                            Burst Time.</li>
                        <li>This does not include the waiting time.</li>
                        <li>It is confusing to calculate the execution time for a process even before executing it
                            hence the scheduling problems based on the burst time cannot be implemented in reality.
                        </li>
                    </ul>
                </li>
                <li><b>Completion Time</b>
                    <ul>
                        <li>The time at which the process enters into the completion state or the time at which the
                            process completes its execution.</li>
                    </ul>
                </li>
                <li><b>Turn around time</b>
                    <ul>
                        <li>The total amount of time spent by the process from its arrival to its completion.</li>
                    </ul>
                </li>
                <li><b>Waiting Time</b>
                    <ul>
                        <li>The total amount of time for which the process waits for the CPU to be assigned.</li>
                    </ul>
                </li>
                <li><b>Response Time</b>
                    <ul>
                        <li>The difference between the arrival time and the time at which the process first gets the
                            CPU.</li>
                    </ul>
                </li>
            </ol>
        </div>

        <div id="t5" class="wh">
            <h2>CPU Scheduling</h2>
            <ul>
                <li><b>In the uniprogramming systems</b> like MS DOS, when a process waits for I/O operation to be done,
                    the CPU remains idol.
                    <ul>
                        <li>Uniprogramming is a type of operating system in which only one program can be executed at a
                            time.</li>
                    </ul>
                </li>
                <li>This is an overhead since it wastes the time and causes the problem of starvation.
                    <ul>
                        <li> "overhead" refers to the extra resources that are required by the system to perform tasks
                            beyond what is required for the user's program. Overhead can include the time, memory,
                            processing power, or other system resources that are needed to manage and run the operating
                            system itself.</li>
                    </ul>
                </li>
                <li>However, in <b>multirogramming systems</b>, the CPU doesn't remain idle during the waiting time of
                    the process and it starts executing other processes.</li>
                <li>Operating system has to define which process the CPU will be given.</li>
                <li><b>In multiprogramming systems</b>, the OS schedules the processes on the CPU to have the maximum
                    utilization of it and this procedure is called <b>CPU scheduling</b>. The Operating system uses
                    various scheduling algorithm to schedule the processes.</li>
                <li>This is a task of the short term schedular to schedule the CPU for the number of processes present
                    in ready queue.</li>
                <li>Whenever the running process requests some I/O operation then the short term scheduler saves the
                    current context of the process (also called PCB) and changes its state from running to waiting.
                    During the time, process is in waiting state; the short term shceduler picks another process from
                    the ready queue and assigns the CPU to this process. This procedure is called <b>context
                        switching.</b></li>
            </ul>
            <p>Context switching diagram &darr;</p>
            <img src="../../images/contextswitching.svg" alt="context switching" class="wb">
            <ul>
                <li><a href="https://www.youtube.com/watch?v=R4ePnbmeTd4" target="_blank" class="ba">Context switching
                        diagram
                        video
                        &neArr;</a></li>
            </ul>
            <div class="in">
                <h3>What is saved in the Process Control Block?</h3>
                <ul>
                    <li>The OS maintains a process control block during the lifetime of the process.</li>
                    <li>Process control block is a data structure used in operating system to store all data related
                        information to the process.</li>
                    <li>The process control block is deleted when the process is terminated or killed.</li>
                    <li>There is the following information which is saved in the PCB and is changing with the state of
                        the process.
                        <ol>
                            <li>Process ID &rarr; A unique identifier for the process.</li>
                            <li>Process State &rarr; The current state of the process (running, waiting, etc.)</li>
                            <li>Pointer &rarr; Pointers to the PCBs of the parent and child processes, and other
                                relevant data structures.</li>
                            <li>Priority &rarr; The priority of the process in relation to other processes in the
                                system.</li>
                            <li>Program Counter &rarr; The address of the next instruction to be executed.</li>
                            <li>CPU Registers &rarr; The values of the CPU registers for the process.</li>
                            <li>I/O status information &rarr; Information about the process's open files, pending I/O
                                requests, and other I/O-related data.</li>
                            <li>Accounting Information &rarr; The amount of CPU time used, clock time since process
                                creation, and other information used for process accounting.</li>
                            <li>etc.</li>
                        </ol>
                    </li>
                </ul>
            </div>
            <div class="in">
                <h3>Different crieteria used for CPU scheduling in an operating system:</h3>
                <ol>
                    <li>CPU utilization:This refers to the percentage of time that the CPU is being used. A scheduling
                        algorithm that aims to maximize CPU utilization will try to keep the CPU busy as much as
                        possible.</li>
                    <li>Throughput: This refers to the number of processes that are completed per unit time. A
                        scheduling algorithm that aims to maximize throughput will try to maximize the number of
                        processes that are completed in a given amount of time.</li>
                    <li>Turnaround Time: This is the amount of time it takes for a process to complete, from the moment
                        it enters the ready queue until it finishes executing. A scheduling algorithm that aims to
                        minimize turnaround time will try to minimize the time that processes spend waiting in the ready
                        queue.</li>
                    <li>Waiting Time: This is the amount of time that a process spends waiting in the ready queue. A
                        scheduling algorithm that aims to minimize waiting time will try to minimize the amount of time
                        that processes spend waiting in the ready queue.</li>
                    <li>Response Time: This is the amount of time it takes for a process to start responding after a
                        request has been made. A scheduling algorithm that aims to minimize response time will try to
                        minimize the time that processes spend waiting in the ready queue before they start executing.
                    </li>
                </ol>
            </div>
        </div>
        <div id="t6" class="wh">
            <ul>
                <li>There are various algorithms which are used by the OS to schedule the processes on the processor in
                    an efficient way.</li>
            </ul>
            <p><b>The purpose of a Scheduling algorithm</b>
            <ol>
                <li>Maximum CPU utilization</li>
                <li>Fare allocation of CPU</li>
                <li>Maximum throughput</li>
                <li>Minimum turnaround time</li>
                <li>Minimum waiting time</li>
                <li>Minimum response time</li>
            </ol>
            </p>
            <p>There are the following algorithms which can be used to schedule the process.</p>
            <ol>
                <li>First Come First Serve</li>
                <li>Shortest Job First</li>
                <li>Round Robin</li>
            </ol>
        </div>
        <div id="t7" class="wh">
            <h2>First Come First Server CPU Process Scheduling in OS</h2>
            <ul>
                <li>This is the basic algorithm which every student must learn to understand all the basics of CPU
                    Process Scheduling Algorithms.</li>
                <li>First Come First Serve paves the way for understanding of other algorithms.</li>
                <li>This algorithm may have many disadvantages, but these disadvantages created very new and
                    efficient algorithms. So, we should learn First Come First Serve first.</li>
                <li><b>Important Abbreviations:</b>
                    <ol>
                        <li>CPU &rarr; Central Processing Unit</li>
                        <li>FCFS &rarr; First Come First Serve</li>
                        <li>AT &rarr; Arrival Time</li>
                        <li>BT &rarr; Burst Time</li>
                        <li>WT &rarr; Waiting Time</li>
                        <li>TAT &rarr; Turn Around Time</li>
                        <li>CT &rarr; Completion Time</li>
                        <li>FIFO &rarr; First In First Out</li>
                    </ol>
                </li>
            </ul>
            <div class="in">
                <h3>First Come First Serve</h3>
                <ul>
                    <li>FCFS is the first algorithm of CPU process scheduling algorithm.</li>
                    <li>FCFS allow the process to execute in linear manner.</li>
                    <li>This means that whichever process enters the ready queue first is executed first.</li>
                    <li>This shows that FCFS algorithm follows FIFO principle.</li>
                    <li>The FCFS algorithm can be executed in Pre Emptive and Non Pre Emptive manner.</li>
                </ul>
            </div>
            <div class="in">
                <h3>Preemptive Approach</h3>
                <ul>
                    <li>Here the OS allots the resources to a process for a predetermined period of time.</li>
                    <li>The process transitions from running state to ready state or from waiting state to ready
                        state during resource allocation.</li>
                    <li>This switching happens because the CPU may assign other processes precedence and substitute
                        the currently active process for the higher priority process.</li>
                </ul>
            </div>
            <div class="in">
                <h3>Non Preemptive Approach</h3>
                <ul>
                    <li>Here the resource cannot be withdrawn from a process before the process has finished
                        running.</li>
                    <li>When a running process finishes and transitions to the waiting state, resources are
                        switched.</li>
                </ul>
            </div>
            <div class="in">
                <h3>Convoy Effect in FCFS</h3>
                <ul>

                    <li>Convoy effect is a phenomenon which occurs in the scheduling algorithm named FCFS.</li>
                    <li>The convoy effect occurs when the FCFS Scheduling algorithm occurs in a way of non preemptive
                        way.</li>
                    <li>The non preemptive way means that if a process or job is started execution, then the
                        operating system must complete is process or job. </li>
                    <li>Until, the process is zero the next process does not start its execution.</li>
                    <li>The definition of Non Preemptive scheduling in terms of OS means that the CPU will be
                        completely dedicated till the end of the process started first and the new process is
                        executed only after finishing of the older process.</li>
                    <li>There may be a few cases, which might cause the CPU to allot a too much time. This is
                        because in the FCFS sheduling algorithm non preemptive approach, the process are chose in
                        serial oder. Due, to this shorter process behind the larger process takes too much time to
                        complete its execution. Due, to this the WT, TAT, CT is very high.</li>
                    <li>If the first process if large or completion time is too high, then convoy effect in the FCFS
                        algorithm is occured.</li>
                    <li>Let us assume that longer job takes infinite time to complete. Then, the remaining processes
                        have to wait for the same infinite time. Due to this convoy effect created by the longer job
                        the starvation of the waiting processes increases very rapidly. this is the biggest
                        disadvantage of FCFS CPU process scheduling.</li>
                </ul>
            </div>
            <div class="in">
                <h3>Characteristics of FCFS CPU Process Scheduling</h3>
                <ol>
                    <li>Implementation is simple.</li>
                    <li>Does not cause any casualties while using.</li>
                    <li>It adopts a non pre-emptive and pre-emptive strategy.</li>
                    <li>It runs each procedure in the order that they are recieved.</li>
                    <li>Arrival time is used as a selection criterin for procedures.</li>
                </ol>
            </div>
            <div class="in">
                <h3>Advantages of FCFS CPU Process Scheduling</h3>
                <ol>
                    <li>In order to allocate processes, it uses the FIFO queue.</li>
                    <li>The FCFS CPU Scheduling Process is straight forward and easy to implement.</li>
                    <li>In the FCFS situation pre emptive scheduling, there is not chance of process starving.</li>
                    <li>As there is no consideration of process priority, it is an equitable algorithm.</li>
                </ol>
            </div>
            <div class="in">
                <h3>Disadvantages of FCFS CPU Process Scheduling</h3>
                <ul>
                    <li>FCFS CPU Scheduling Algorithm has Long Waiting Time.</li>
                    <li>FCFS CPU Scheduling favors CPU over Input or Output operations.</li>
                    <li>In FCFS there is a chance of occurrence of Convoy Effect.</li>
                    <li>Because FCFS is so straight forward, it often isn't very effective. Extended waiting periods
                        go hand in hand with this. All other orders are left idle if the CPU is busy processing one
                        time-consuming order.</li>
                </ul>
            </div>
            <div class="in">
                <h3>Problems on the FCFS CPU Scheduling Algorithm</h3>
                <img src="../../images/numerical1.svg" alt="" class="wb">
                <p>This is how the FCFS is solved in Non Pre Emptive Approach.</p>
            </div>
        </div>
        <div id="t8" class="wh">
            <h2>Shortest Job First (SJF) Scheduling</h2>
            <ul>
                <li>SJF Scheduling algorithm, schedules the processes according to their burst time.</li>
                <li>In SJF scheduling, the process with the lowest burst time, among the list of available processes in
                    the ready queue, is going to be scheduled next.</li>
                <li>However, it is very difficult to predict the burst time needed for a process hence this algorithm is
                    very difficult to implement in the system.</li>
            </ul>
            <div class="in">
                <h3>Advantages of SJF</h3>
                <ul>
                    <li>Maximum throughput</li>
                    <li>Minimum average waiting and turnaround time</li>

                </ul>
            </div>
            <div class="in">
                <h3>Disadvantages of SJF</h3>
                <ul>
                    <li>May suffer with the problem of starvation.</li>
                    <li>It is not implementable because the exact burst time for a process can't be known in advance.
                    </li>

                </ul>
            </div>
            <div class="in">
                <h3>Non-Preemptive shortest job first CPU Scheduling Algorithm example:</h3>
                <img src="../../images/numerical2.svg" alt="" class="wb">
                <ul>
                    <li><b>Advantages:</b>
                        <ul>
                            <li>SJF is better than the FCFS algorithm as it reduces the average waiting time.</li>
                            <li>SJF is genrally used for long term scheduling.</li>
                            <li>It is suitable for the jobs running in batches, where run times are already known.</li>
                            <li>SJF is probably optimal in terms of average turnaround time.</li>
                        </ul>
                    </li>
                    <li><b>Disadvantages:</b>
                        <ul>
                            <li>SJF may cause very long turn-around times or starvation.</li>
                            <li>In SJF job completion timme must be known earlier, but sometimes it is hard to predict.
                            </li>
                            <li>Sometimes, it is complicated to predict the length of the upcoming CPU request.</li>
                            <li>It leads to the starvation that does not reduce average turnaround time.</li>
                        </ul>
                    </li>
                </ul>
            </div>
            <div class="in">
                <h3>Preemptive SJF - Shortest Remaining Time First Scheduling Algorithm</h3>
                <ul>
                    <li>In SRTF schefuling algorithm, the process with the smallest amount of time remaining until
                        completion is selected to execute.</li>
                    <li>Since the currently executing process is the one with the shortest amount of time remaining by
                        definition, and since that time should only reduce as execution progresses, process will always
                        run until they complete or a new process is added that requires a smaller amount of time.</li>
                </ul>
                <p>Examples to show working of Preemptive Shortest Job First CPU Scheduling Algorithm &darr;</p>
                <img src="../../images/numerical3.svg" alt="" class="wb">
            </div>
        </div>
        <div id="t9" class="wh">
            <h2>Round Robin Scheduling Algorithm</h2>
            <ul>
                <li>In RR algorithm each process is assigned a fixed time slot in a cyclic way.</li>
                <li>It is basically the preemptive version of FCFS scheduling algorithm.</li>
                <li>RR CPU algorithm generally focuses on Time sharing technique.</li>
                <li>The period of time for which a process or job is allowed to run in a pre-emptive method is called
                    time quantum.</li>
                <li>Each process present in the ready queue is assigned the CPU for that time quantum, if the execution
                    of the process is completed during that time then the process will end else the process will go back
                    to the waiting table and wait for its next turn to complete the execution.</li>
            </ul>
            <div class="in">
                <h3>Characteristics of RR:</h3>
                <ul>
                    <li>It is simle, easy to implement, and starvation-free as all process get fair share of CPU time.
                    </li>
                    <li>One of the most commonly used technique in CPU scheduling as a core.</li>
                    <li>It is preempitve as process are assigned CPU only for a fixed slice of time at most.</li>
                    <li>The disadvantages of it is more overhead of context switching.</li>
                </ul>
            </div>
            <div class="in">
                <h3>Advantages of RR:</h3>
                <ul>
                    <li>There is fairness since every process gets equal share of CPU.</li>
                    <li>The newly created process is added to end of ready queue.</li>
                    <li>A round-robin scheduler generally employs time-sharing, giving each job a time slot
                        or quantum.</li>
                    <li>While performing a round-robin scheduling, a particular time quantum is allotted to
                        different jobs.</li>
                    <li>Each process get a chance to reschedule after a particular quantum time in this
                        scheduling</li>
                </ul>
            </div>
            <div class="in">
                <h3>Disadvantages of RR:</h3>
                <ul>
                    <li>There is Larger waiting time and Response time.</li>
                    <li>There is Low throughput</li>
                    <li>There is Context Switches</li>
                    <li>Gantt chart seems to come too big (if quantum time is less for scheduling.</li>
                    <li>Time consuming scheduling for small quantum</li>
                </ul>
            </div>
            <div class="in">
                <p><b>Consider the following table of arrival time and burst time for four processes P1, P2,
                        P3, and P4 and given Time Quantum = 2. &darr;</b></p>
                <img src="../../images/numerical4.svg" alt="" class="wb">
                <ul>
                    <li><a href="https://www.youtube.com/watch?v=6PEyXwdxeIc&t=557s" class="ba" target="_blank">Helping
                            video
                            &neArr;</a></li>
                </ul>
            </div>
        </div>
        <div id="t10" class="wh">
            <h2>Multiple-Processor Scheduling</h2>
            <ul>
                <li>In multiple-processor scheduling multiple CPU's are available and hence <b>load sharing</b> becomes
                    possible.</li>
                <li>However multiple processor scheduling is more complex as compared to single processor scheduling.
                </li>
                <li>In multiple processor scheduling there are cases when the processors are identical i.e. HOMOGENEOUS,
                    in terms of their functionality, we can use any processor available to run any process in the queue.
                </li>
            </ul>
            <div class="in">
                <h3>Approaches to Multiple-Processor Scheduling:</h3>
                <ul>
                    <li>One approach is when all the scheduling decisions and I/O processing are handled by a single
                        processor which is called the <b>master server</b> and the other processors executes only the
                        <b>user code</b>. This is simple and reduces the need of data sharing. This entire scenario is
                        called <b>asymmetric multiprocessing.</b>
                    </li>
                    <li>A second approach uses <b>symmetric multiprocessing</b> where each processor is self scheduling.
                        All processes may be in common ready queue or each processor may have its own private queue for
                        ready processes. The scheduling proceeds further by having the scheduler for each processor
                        examine the ready queue and select a process to execute.</li>
                </ul>
            </div>
        </div>
        <div id="t11" class="wh">
            <h2>Processor Affinity</h2>
            <ul>
                <li>Processor affinity is the ability to bind a process or thread to a specific CPU or core in a
                    multi-core system. This means that the process or thread will be scheduled to run exclusively on the
                    chosen CPU or core, and will not be allowed to run on any other CPU or core in the system.</li>
                <li>The purpose of processor affinity is to improve the performance of a system by reducing the overhead
                    of context switching between different CPUs or cores. By binding a process or thread to a specific
                    CPU or core, the operating system can reduce the time and resources needed to switch between
                    different CPUs or cores, which can result in faster execution times and more efficient use of system
                    resources.</li>
                <li>When a process runs on a specific processor there are certain effects on the cache memory. The
                    data most recently accessed by the process populate the cache for the processor and as a result
                    successive memory access by the process are often satisfied in the cache memory. Now if the
                    process migrates to another processor, the contents of the cache memory must be invalidated for
                    the first processor and the cache for the second processor must be repopulated. Because of the
                    high cost of invalidating and repopulating caches, most of the SMP(symmetric multiprocessing)
                    systems try to avoid migration of processes from one processor to another and try to keep a
                    process running on the same processor. This is known as PROCESSOR AFFINITY.</li>
            </ul>
            <div class="in">
                <h3>There are two types of processor affinity:</h3>
                <ol>
                    <li><b>Soft Affinity:</b> When an operating system has a policy of attempting to keep a process
                        running on the same processor but not guaranteeing it will do so, this situation is called soft
                        affinity.</li>
                    <li><b>Hard Affinity:</b> Hard affinity allows a process to specify a subset of processors on which
                        it may run. Some systems such as Linux implements soft affinity but also provide some system
                        calls like <i>sched_setaffinity()</i> that supports hard affinity.</li>
                </ol>
            </div>
        </div>
        <div id="t12" class="wh">
            <h2>Load Balancing</h2>
            <ul>
                <li>Load balancing is a phenomena which keeps the workload evenly distributed accross all processors in
                    a SMP system. Load balancing is necessary only on systems where each processors has its own private
                    queue of process which are eligible to execute.</li>
                <li>Load balancing is unncessary because once a processor becomes idle it immediately extracts a
                    runnable process from the common run queue.</li>
                <li>On SMP, it is important to keep the workload balanced among all processors to fully utilize the
                    benefits of having more than one processor else one or more processor will sit idle while other
                    processors have high workloads along with lists of processors awaiting the CPU.</li>
            </ul>
            <div class="in">
                <h3>There are two general approaches to load balancing:</h3>
                <ol>
                    <li><b>Push Migration:</b> In push migration a task routinely checks the load on each
                        processor and if it finds an imbalance then it evenly distributes load on each
                        processors by moving the processes from overloaded to idle or less busy processors.</li>
                    <li><b>Pull Migration:</b> Pull Migration occurs when an idle processor pulls a waiting task
                        from a busy processor for its execution.</li>
                </ol>
            </div>
        </div>
        <div id="t13" class="wh">
            <h2>Multicore Processors</h2>
            <ul>
                <li>In multicore processors <b>multiple processor</b> core are placed on some physical chip.</li>
                <li>Each core has a register set to maintain its architectural state and thus appears to the operating
                    system as a separate physical processor.</li>
                <li><b>SMP systems</b> that use multicore processors are faster and consume less power than systems in
                    which each processor has its own physical chip.</li>
                <li>However multicore processors may complicate the scheduling problems.</li>
                <li>When processor accesses memory then it spends a significant amount of time waiting for the data to
                    become available, this situation is called <b>memory stall.</b> It occurs for various reasons such
                    as cache miss, which is accessing the data that is not in the cache memory. In such cases the
                    processor can spen upto fifty percent of its time waiting for data to become available from the
                    memory. To solve this problem recent hardware designs have implemented multithreaded processor cores
                    in which two or more hardware threads are assigned to each core. Therefor if one thread stalls while
                    waiting for the memory, core can switch to another thread.</li>
            </ul>
        </div>
        <div id="t14" class="wh">
            <h2>Process and Thread</h2>
            <ul>
                <li><b>Process:</b>
                    <ol>
                        <li>Processes are basically the programs that are dispatched from the ready state and are
                            scheduled in the CPU for execution. PCB(Process Control Block) holds the concept of process.
                            A process can create other processes which are known as Child Processes. The process takes
                            more time to terminate and it is isolated means it does not share the memory with any other
                            process.</li>
                        <li>The process can have the following states: new, ready, running, waiting, terminated, and
                            suspended</li>
                    </ol>
                </li>
                <li><b>Thread:</b>
                    <ol>
                        <li>Thread is the segment of a process which means a process can have multiple threads
                            and these multiple threads are contained within a process. </li>
                        <li>A thread has three states: Ready, Running and Blocked.</li>
                        <li>The thread takes less time to terminate as compared to the process but unlike the process,
                            threads
                            do not isolate.</li>
                    </ol>
                </li>
            </ul>
            <div class="in">
                <h3>Difference between Process and Thread:</h3>
                <img src="../../images/thread1.svg" alt="" class="wb">
            </div>
        </div>
        <div id="t15" class="wh">
            <h2>Multithreading in OS</h2>
            <ul>
                <li>Multithreading is a technique used in operating systems to enable multiple threads of execution
                    within a single process. A thread is a lightweight process that shares the same memory space and
                    resources as other threads in the same process. Multithreading allows for concurrent execution of
                    multiple tasks within a single process, making it possible to achieve better performance and
                    responsiveness in certain types of applications</li>
                <li>Lets say, for example a program is not capable of reading keystrokes while making drawings. These
                    tasks cannot be executed by the program at the same time. This problem can be solved through
                    multi-tasking so that two or more tasks can be executed simultaneously.</li>
                <li>One of the benefits of multithreading is that it can improve overall system responsiveness by
                    allowing applications to continue running even when some threads are blocked or waiting for I/O
                    operations to complete. Multithreading can also help to reduce the overhead associated with context
                    switching between processes, since switching between threads within a process is typically faster.
                </li>
                <li>The concept of multi-threading needs proper understanding of these two terms – a process and
                    a thread.
                    <ul>
                        <li> A process is a program being executed. A process can be further divided into
                            independent units known as threads.</li>
                        <li>A thread is like a small light-weight process within a process.
                            Or we can say a collection of threads is what is known as a
                            process.</li>
                    </ul>
                </li>
                <li>Multi-tasking is of two types:
                    <ol>
                        <li>Processor based: It is totally managed by the OS.</li>
                        <li>Thread based: It can be controlled by the programmer to some extent.</li>
                    </ol>
                </li>
            </ul>
            <img src="../../images/thread2.svg" alt="" class="wb">
            <div class="in">
                <h3>Applicaitons</h3>
                <p>Threading is a technique used in computer programming to improve the performance and responsiveness
                    of software applications. It involves dividing a program into multiple smaller threads or tasks that
                    can be executed independently and concurrently</p>
                <ul>
                    <li>Threading is used widely in almost every field.</li>
                    <li>Most widely it is seen over the internet nowadays where we are using transaction processing of
                        every type like reacharges, online transfer, banking etc.</li>
                    <li>GUI Applications: Threading is commonly used in Graphical User Interface (GUI) applications to
                        keep the interface responsive while performing computationally intensive tasks. For example,
                        when you click a button in a GUI application, a separate thread can be created to perform the
                        task while the main thread keeps the interface responsive.</li>
                    <li>Network Applications: Threading can be used in network applications to improve performance and
                        scalability. For example, a server application can use multiple threads to handle incoming
                        client connections simultaneously.</li>
                    <li>Multimedia Applications: Threading is commonly used in multimedia applications to play audio and
                        video files while simultaneously performing other tasks. For example, a media player can use a
                        separate thread to play the media file while the main thread handles user input and interface
                        updates.</li>
                    <li>Parallel Processing: Threading can be used in parallel processing applications to speed up
                        computations by dividing a large task into smaller independent tasks that can be executed
                        simultaneously on different threads.</li>
                    <li>Games: Threading can be used in games to keep the game running smoothly while performing tasks
                        such as physics calculations, artificial intelligence, and rendering</li>
                </ul>
            </div>
        </div>
    </div>
    <div id="t16" class="content-box">
        <h2>Previous Year Questions</h2>
        <div class="wh">
            <h3>Q- What do you mean by PCB? Where is it used? What are its contents? Explain.</h3>
        </div>
        <div class="wh">
            <h3>Q- Distinguish between the following:</h3>
            <ul>
                <li>Process and Program</li>
                <li>Multiprogramming and multiprocessing</li>
                <li>Job scheduling and CPU scheduling</li>
            </ul>
        </div>
        <div class="wh">
            <h3>Q- Consider four process P1, P2, P3 and P4 with arrival time (0, 2, 4, 5) and burst time (7, 4, 1, 4)
                respectively, what is the average waiting time and turnaround time in SJF and Round Robin scheduling
                algorithm (with time quantum of 2ns) respectively?</h3>
        </div>
        <div class="wh">
            <h3>Q- Draw the process state diagram and explain functionality of each state in detail.</h3>
        </div>
        <div class="wh">
            <h3>Q- Explain the following:</h3>
            <ol>
                <li>Multithreading</li>
                <li>Context switching</li>
                <li>Schedulers</li>
            </ol>
        </div>
    </div>
    <div class="content-box">
        <h2>Assignment question answer</h2>
        <div class="wh">
            <p><b>1.</b> Some CPUs provide for more than two modes of operation. What are two possible uses of these
                multiple modes?</p>
            <p><b>Ans.</b> Multiple modes of operation in CPUs can be used for various purposes, including:
            <ol>
                <li>Privilege separation: Some CPUs provide multiple modes of operation to separate the privilege
                    levels of software running on the system. This enables the system to provide different levels of
                    access to resources and hardware devices based on the privilege level of the software. For
                    example, operating systems can use different modes of operation to protect system resources and
                    prevent unauthorized access to them.</li>
                <li>Virtualization: Multiple modes of operation can also be used to enable virtualization, where
                    multiple virtual machines (VMs) can run on a single physical machine. In this scenario, each VM
                    can run in its own mode of operation, isolating it from other VMs and the host system. This
                    provides increased security and flexibility, as each VM can run its own operating system and
                    software stack, independently of other VMs on the same machine.</li>
            </ol>
            </p>
        </div>
        <div class="wh">
            <p><b>2.What are the three major activities of an operating system with regard to memory management?</b>
            </p>
            <p><b>Ans.</b> The three major activities of an operating system with regard to memory management are:
            <ol>
                <li>
                    Allocation: The operating system is responsible for allocating memory to different processes and
                    applications running on the system. When a process or application requests memory, the operating
                    system determines whether there is enough free memory available and, if so, allocates a portion
                    of memory to the process.
                </li>
                <li>Deallocation: The operating system is also responsible for deallocating memory that is no longer
                    needed by a process. When a process terminates or no longer requires a portion of memory that it
                    had previously allocated, the operating system deallocates that memory and makes it available
                    for other processes to use.</li>
                <li>Protection: The operating system must ensure that each process can access only the memory that
                    it has been allocated and prevent processes from accessing memory that belongs to other
                    processes. The operating system uses techniques such as address translation and memory
                    protection mechanisms to enforce memory protection and prevent processes from accessing
                    unauthorized memory locations.</li>
            </ol>
            </p>
        </div>
        <div class="wh">
            <p><b>3. Provide two programming examples in which multi-threading does not provide better performance
                    than a single-threaded solution</b></p>
            <p><b>Ans.</b> There are situations in which multithreading does not provide better performance than a
                single-threaded solution. Two programming examples are:
            <ol>
                <li>Small tasks: If the tasks that need to be performed are very small, the overhead of creating and
                    managing multiple threads can actually outweigh the benefits of parallelization. For example,
                    consider a program that needs to sort an array of 10 elements. In this case, creating multiple
                    threads to sort the array would likely result in slower performance than simply sorting the
                    array in a single thread.</li>
                <li>Shared resources: If the threads in a multithreaded program need to access shared resources such
                    as memory or files, managing access to these resources can become a bottleneck and limit the
                    benefits of parallelization. For example, consider a program that reads data from a file,
                    processes it, and writes the results to another file. If multiple threads are used to perform
                    the processing, they may contend for access to the input and output files, leading to reduced
                    performance compared to a single-threaded solution that processes the data sequentially.</li>
            </ol>In both of these examples, the overhead of creating and mananging can outweigh the benefits of
            parallelizaiton, leading to slower performance than a single-threaded solution.</p>
        </div>
    </div>
    <script src="../../../../public/main.js"></script>
</body>

</html>